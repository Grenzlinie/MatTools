{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construct code based Document object -- method1\n",
    "# load source codes and split them into chunks\n",
    "from langchain_text_splitters import (\n",
    "    Language,\n",
    "    RecursiveCharacterTextSplitter,\n",
    ")\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_community.document_loaders.generic import GenericLoader\n",
    "from langchain_community.document_loaders.parsers import LanguageParser\n",
    "import re\n",
    "from langchain_core.documents import Document\n",
    "from call_llms import load_embedding_model\n",
    "base_path = \"tool_source_code/pymatgen/src/pymatgen/\"\n",
    "loader = GenericLoader.from_filesystem(base_path, glob=\"**/[!.]*\", suffixes=[\".py\"], parser=LanguageParser(language=\"python\"))\n",
    "pymatgen_source_docs = loader.load()\n",
    "def update_metadata_with_first_function_or_class(document: Document):\n",
    "    \"\"\"\n",
    "    Update the document's metadata to add the name of the first occurring class or function.\n",
    "    \n",
    "    Args:\n",
    "        document: A document object containing metadata and page_content.\n",
    "    \n",
    "    Returns:\n",
    "        The updated document object.\n",
    "    \"\"\"\n",
    "    # check content_type of metadata\n",
    "    page_content = document.page_content\n",
    "        \n",
    "    # Define a regular expression to match function and class names\n",
    "    pattern = r\"^\\s*(def|class)\\s+([a-zA-Z_][a-zA-Z0-9_]*)\"\n",
    "    match = re.search(pattern, page_content, re.MULTILINE)\n",
    "        \n",
    "    if match:\n",
    "        # extract the first function or class name with its type\n",
    "        type_name = match.group(1)\n",
    "        first_name = match.group(2)\n",
    "        # add it to metadata with the type prefix\n",
    "        document.metadata[\"first_function_or_class\"] = f\"{type_name} {first_name}\"\n",
    "    else:\n",
    "        failed_path = document.metadata['source']\n",
    "        print(f\"No function or class found in the document {failed_path}.\")\n",
    "for doc in pymatgen_source_docs:\n",
    "    if doc.metadata[\"content_type\"] == \"functions_classes\":\n",
    "        update_metadata_with_first_function_or_class(doc)\n",
    "    # Find the second 'pymatgen' part in the path and keep everything after it, including 'pymatgen'\n",
    "    doc.metadata[\"source\"] = \"pymatgen\" + doc.metadata[\"source\"].split(\"pymatgen\", 2)[-1]\n",
    "print(f\"Loaded {len(pymatgen_source_docs)} source codes.\")\n",
    "python_splitter = RecursiveCharacterTextSplitter.from_language(language=Language.PYTHON, chunk_size=1000, chunk_overlap=50)\n",
    "pymatgen_splitted_docs = python_splitter.split_documents(pymatgen_source_docs)\n",
    "print(f\"Codes are split into {len(pymatgen_splitted_docs)} chunks.\")\n",
    "embedding_model = load_embedding_model(\"text-embedding-3-large\")\n",
    "vector_store = Chroma(collection_name='pymatgen', embedding_function=embedding_model, persist_directory=\"vector_store/vs_method1/\")\n",
    "# add documents to vector store\n",
    "vector_store.add_documents(documents=pymatgen_splitted_docs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# documents from official documentation -- method2\n",
    "from langchain_community.document_loaders import BSHTMLLoader\n",
    "import os\n",
    "from pathlib import Path\n",
    "from langchain.document_loaders import TextLoader\n",
    "from langchain_core.documents import Document\n",
    "from call_llms import load_embedding_model\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "import tiktoken\n",
    "from langchain_chroma import Chroma\n",
    "\n",
    "def calculate_token_statistics(documents: list, encoding_name: str = \"cl100k_base\") -> dict:\n",
    "    \"\"\"\n",
    "    计算文档列表中每个文档的token数量并返回统计信息\n",
    "    \n",
    "    Args:\n",
    "        documents: 包含Document对象的列表\n",
    "        encoding_name: 使用的编码名称，默认为\"cl100k_base\"\n",
    "    \n",
    "    Returns:\n",
    "        包含token统计信息的字典，包括token_counts, max_tokens, min_tokens\n",
    "    \"\"\"\n",
    "    encoding = tiktoken.get_encoding(encoding_name)\n",
    "    token_counts = [len(encoding.encode(doc.page_content)) for doc in documents]\n",
    "    \n",
    "    return {\n",
    "        \"token_counts\": token_counts,\n",
    "        \"max_tokens\": max(token_counts),\n",
    "        \"min_tokens\": min(token_counts)\n",
    "    }\n",
    "\n",
    "def get_pymatgen_html_files(directory: str):\n",
    "    \"\"\"\n",
    "    获取指定目录下所有以 'pymatgen' 开头的 HTML 文件和 'modules.html' 文件的路径，\n",
    "    但排除 '_modules' 目录下的文件。\n",
    "    \n",
    "    参数：\n",
    "    directory (str): 目标目录路径，例如 'tool_source_code/pymatgen/docs'\n",
    "    \n",
    "    返回：\n",
    "    list: 符合条件的文件路径列表\n",
    "    \"\"\"\n",
    "    html_files = []\n",
    "    \n",
    "    if not os.path.exists(directory):\n",
    "        print(f\"Directory {directory} does not exist.\")\n",
    "        return html_files\n",
    "    \n",
    "    for root, _, files in os.walk(directory):\n",
    "        # 排除 _modules 目录\n",
    "        if \"_modules\" in root.split(os.sep):\n",
    "            continue\n",
    "\n",
    "        for file in files:\n",
    "            if (file.startswith(\"pymatgen\") and file.endswith(\".html\")) or file == \"modules.html\":\n",
    "                html_files.append(os.path.join(root, file))\n",
    "    \n",
    "    return html_files\n",
    "\n",
    "html_files_list = get_pymatgen_html_files(\"tool_source_code/pymatgen/docs\")\n",
    "print(len(html_files_list))\n",
    "\n",
    "documents = [BSHTMLLoader(file).load() for file in html_files_list]\n",
    "\n",
    "documents = [doc for sublist in documents for doc in sublist]\n",
    "\n",
    "print(f\"Loaded {len(documents)} documents from local html files.\")\n",
    "\n",
    "token_stats = calculate_token_statistics(documents)\n",
    "print(\"Token counts:\", token_stats[\"token_counts\"])\n",
    "print(\"Max tokens:\", token_stats[\"max_tokens\"])\n",
    "print(\"Min tokens:\", token_stats[\"min_tokens\"])\n",
    "\n",
    "# 指定文件夹路径\n",
    "folder_path = \"./defects_doc/\"  # 替换为实际路径\n",
    "\n",
    "# 存储所有 Document\n",
    "documents_defect = []\n",
    "\n",
    "# 遍历文件夹中的所有 txt 文件（不考虑子文件夹）\n",
    "for file in Path(folder_path).glob(\"*.txt\"):\n",
    "    # 加载文件内容\n",
    "    loader = TextLoader(str(file))\n",
    "    docs = loader.load()\n",
    "\n",
    "    # 获取文件名（不带扩展名）\n",
    "    file_name = file.stem\n",
    "\n",
    "    # 处理文件名\n",
    "    if file_name == \"api_overview\":\n",
    "        title = \"api overview — 2024.7.19 documentation\"\n",
    "    else:\n",
    "        title = file_name.replace(\"_\", \".\") + \" package — 2024.7.19 documentation\"\n",
    "\n",
    "    # 创建新的 Document 并添加到列表\n",
    "    for doc in docs:\n",
    "        documents_defect.append(Document(\n",
    "            page_content=doc.page_content,\n",
    "            metadata={\"source\": str(file).split(\"defects_doc/\")[1], \"title\": title}\n",
    "        ))\n",
    "        \n",
    "print(f\"Loaded {len(documents_defect)} documents from local txt files.\")\n",
    "token_stats = calculate_token_statistics(documents_defect)\n",
    "print(\"Token counts:\", token_stats[\"token_counts\"])\n",
    "print(\"Max tokens:\", token_stats[\"max_tokens\"])\n",
    "print(\"Min tokens:\", token_stats[\"min_tokens\"])\n",
    "\n",
    "documents.extend(documents_defect)\n",
    "print(f\"Total {len(documents)} documents.\")\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=50)\n",
    "split_docs = text_splitter.split_documents(documents)\n",
    "print(f\"Documents are split into {len(split_docs)} chunks.\")\n",
    "embedding_model = load_embedding_model(\"text-embedding-3-large\")\n",
    "vector_store = Chroma(collection_name='pymatgen-doc', embedding_function=embedding_model, persist_directory=\"vector_store/vs_method2/\")\n",
    "vector_store.add_documents(documents=split_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construct llm-generated document vector store with SemanticChunker -- method3\n",
    "from json_handler import JsonFileProcessor\n",
    "from langchain_chroma import Chroma\n",
    "from call_llms import load_embedding_model\n",
    "from langchain_experimental.text_splitter import SemanticChunker\n",
    "db_path = \"/Users/siyuliu/Desktop/pymatgen_source_code_gemini_2_0_flash/.project_doc_record/project_hierarchy.json\"\n",
    "\n",
    "md_contents, meta_data = JsonFileProcessor(db_path).extract_data()\n",
    "\n",
    "embeddings = load_embedding_model(model_name=\"text-embedding-3-large\")\n",
    "\n",
    "if not md_contents or not meta_data:\n",
    "    print(\"No content or metadata provided. Skipping vector store creation.\")\n",
    "    raise ValueError\n",
    "\n",
    "# Ensure lengths match\n",
    "min_length = min(len(md_contents), len(meta_data))\n",
    "md_contents = md_contents[:min_length]\n",
    "meta_data = meta_data[:min_length]\n",
    "\n",
    "semantic_chunker = SemanticChunker(\n",
    "    embeddings=embeddings,\n",
    "    buffer_size=1,\n",
    "    breakpoint_threshold_amount=95,\n",
    ")\n",
    "split_docs = semantic_chunker.create_documents(\n",
    "    texts=md_contents, \n",
    "    metadatas=meta_data\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from langchain.schema import Document\n",
    "\n",
    "# 序列化为 JSON 结构\n",
    "documents_json = [\n",
    "    {\"page_content\": doc.page_content, \"metadata\": doc.metadata} for doc in split_docs\n",
    "]\n",
    "\n",
    "# 写入 JSON 文件\n",
    "with open(\"documents_llm_doc.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(documents_json, f, ensure_ascii=False, indent=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Adding documents: 100%|██████████| 181/181 [08:18<00:00,  2.75s/it]\n"
     ]
    }
   ],
   "source": [
    "from langchain_chroma import Chroma\n",
    "import os\n",
    "from filelock import FileLock\n",
    "from tqdm import tqdm\n",
    "\n",
    "persist_directory = \"vector_store/vs_method3/\"\n",
    "os.makedirs(persist_directory, exist_ok=True)\n",
    "\n",
    "lock_path = os.path.join(persist_directory, \"vector_store.lock\")\n",
    "batch_size = 100\n",
    "\n",
    "with FileLock(lock_path, timeout=120):\n",
    "    # 初始化或加载已有的 Chroma 存储\n",
    "    vector_store = Chroma(\n",
    "        embedding_function=embeddings,\n",
    "        collection_name=\"pymatgen_llm_doc\",\n",
    "        persist_directory=persist_directory\n",
    "    )\n",
    "\n",
    "    for i in tqdm(range(0, len(split_docs), batch_size), desc=\"Adding documents\"):\n",
    "        batch = split_docs[i:i + batch_size]\n",
    "        vector_store.add_documents(batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Method4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Adding documents: 100%|██████████| 72/72 [03:25<00:00,  2.86s/it]\n"
     ]
    }
   ],
   "source": [
    "# llm generated document by full document loading -- method4\n",
    "from json_handler import JsonFileProcessor\n",
    "from langchain_chroma import Chroma\n",
    "from langchain.schema import Document\n",
    "from call_llms import load_embedding_model\n",
    "from filelock import FileLock\n",
    "import json\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "# Load data from JSON\n",
    "db_path = \"/Users/siyuliu/Desktop/pymatgen_source_code_gemini_2_0_flash/.project_doc_record/project_hierarchy.json\"\n",
    "md_contents, meta_data = JsonFileProcessor(db_path).extract_data()\n",
    "\n",
    "# Load embedding model\n",
    "embeddings = load_embedding_model(model_name=\"text-embedding-3-large\")\n",
    "\n",
    "# Validate data\n",
    "if not md_contents or not meta_data:\n",
    "    print(\"No content or metadata provided. Skipping vector store creation.\")\n",
    "    raise ValueError\n",
    "\n",
    "# Ensure lengths match and create Document objects\n",
    "min_length = min(len(md_contents), len(meta_data))\n",
    "documents = [\n",
    "    Document(page_content=md_contents[i], metadata=meta_data[i])\n",
    "    for i in range(min_length)\n",
    "]\n",
    "\n",
    "# 写入 JSON 文件\n",
    "documents_json = [\n",
    "    {\"page_content\": doc.page_content, \"metadata\": doc.metadata} for doc in documents\n",
    "]\n",
    "with open(\"documents_llm_doc_gemini_20_flash_full.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(documents_json, f, ensure_ascii=False, indent=4)\n",
    "\n",
    "\n",
    "\n",
    "# Create Chroma vector store with documents\n",
    "persist_directory = \"vector_store/vs_method4/\"\n",
    "os.makedirs(persist_directory, exist_ok=True)\n",
    "lock_path = os.path.join(persist_directory, \"vector_store.lock\")\n",
    "\n",
    "batch_size = 100\n",
    "\n",
    "\n",
    "with FileLock(lock_path, timeout=120):\n",
    "    vector_store = Chroma(\n",
    "    embedding_function=embeddings,\n",
    "    collection_name=\"pymatgen_llm_doc_full\",\n",
    "    persist_directory=persist_directory\n",
    "    )\n",
    "    \n",
    "    for i in tqdm(range(0, len(documents), batch_size), desc=\"Adding documents\"):\n",
    "        batch = documents[i:i + batch_size]\n",
    "        vector_store.add_documents(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mattoolben",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
